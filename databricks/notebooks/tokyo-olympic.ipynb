{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_name = os.getenv(\"DLS_NAME\")\n",
    "filesystem_stage = os.getenv(\"DLS_FILESYSTEM_STAGE\")\n",
    "tenant_id = os.getenv(\"SP_TENANT_ID\")\n",
    "client_id = os.getenv(\"SP_CLIENT_ID\")\n",
    "\n",
    "service_credential = dbutils.secrets.get(scope=\"keyvault-managed\", key=\"dlsserviceprincipalsecret\")\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_name}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_url = f\"abfss://{filesystem_stage}@{storage_name}.dfs.core.windows.net\"\n",
    "\n",
    "# https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "loader = spark.read.format(\"CSV\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"escape\", \"\\\\\")\\\n",
    "                .option(\"delimiter\", \",\")\n",
    "\n",
    "athletes = loader.load(f\"{lake_url}/Athletes.xlsx.csv\")\n",
    "coaches = loader.load(f\"{lake_url}/Coaches.xlsx.csv\")\n",
    "entries_gender = loader.load(f\"{lake_url}/EntriesGender.xlsx.csv\")\n",
    "medals = loader.load(f\"{lake_url}/Medals.xlsx.csv\")\n",
    "teams = loader.load(f\"{lake_url}/Teams.xlsx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(medals)\n",
    "# athletes.printSchema()\n",
    "# entries_gender.printSchema()\n",
    "# medals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_url = f\"abfss://transformed-data@{storage_name}.dfs.core.windows.net\"\n",
    "\n",
    "# To partition the CSV in multiple files: .repartition(3)\n",
    "athletes.write.format(\"CSV\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "            .save(f\"{transformed_data_url}/athletes\")\n",
    "\n",
    "coaches.write.format(\"CSV\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "            .save(f\"{transformed_data_url}/coaches\")\n",
    "\n",
    "entries_gender.write.format(\"CSV\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "            .save(f\"{transformed_data_url}/entriesgender\")\n",
    "\n",
    "medals.write.format(\"CSV\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "            .save(f\"{transformed_data_url}/medals\")\n",
    "\n",
    "teams.write.format(\"CSV\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "            .save(f\"{transformed_data_url}/teams\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
